{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UK8gkQBEJAPr",
        "Ay1CBq96IbNi"
      ],
      "authorship_tag": "ABX9TyOGLUUquMzE2wwXVu/Xm9iE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moaaz12-web/RAG-using-Langchain-OpenAI-and-Huggingface/blob/main/Langchain_web_search_with_agents%2C_memory_and_per_user_retrieval_using_Redis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook explores a wide variety of topics. They include:\n",
        "1. Web search using langchain agents and serpAPI,\n",
        "2. Memory based LLM chatbot using Langchain and Redis Database,\n",
        "3. Memory based LLM chatbot using Langchain and Redis database acting as a vector store."
      ],
      "metadata": {
        "id": "U4c-C8cGDEkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install sentence-transformers\n",
        "! pip install langchainhub\n",
        "!pip install -U duckduckgo-search\n",
        "! pip install langchain openai tiktoken langchain_openai\n",
        "!pip install \"redis[hiredis]\"\n",
        "!pip install --upgrade --quiet  huggingface_hub"
      ],
      "metadata": {
        "id": "wkvcL6-D9OWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set envs"
      ],
      "metadata": {
        "id": "UK8gkQBEJAPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = \"YOUR KEY\"\n",
        "os.environ[\"REDIS_URL\"] =\"RDIS URL\"\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"YOUR HF TOKEN\"\n",
        "os.environ['SERPAPI_API_KEY'] = 'your key'"
      ],
      "metadata": {
        "id": "DMbYFuK9I_iI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. SIMPLE WEB SEARCH\n",
        "\n",
        "The code below uses langchain agent to perform simple web searches. Typical langchain üôÇüôÇ"
      ],
      "metadata": {
        "id": "hQJ8ugTeWN_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "\n",
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"  # https://platform.openai.com (Thx Michael from Twitter)\n",
        "\n",
        "\n",
        "def QA(ques):\n",
        "\n",
        "    llm = OpenAI(temperature=0, max_tokens=500)\n",
        "    tool_names = [\"serpapi\"]\n",
        "    tools = load_tools(tool_names)\n",
        "    agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=False)\n",
        "    return agent.run(ques)\n",
        "\n",
        "\n",
        "print(QA(\"How much money was generated by Taylor Swift in the ERAS\"))\n"
      ],
      "metadata": {
        "id": "6I5Ko0eKtLFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Chat Message History Memory\n",
        "\n",
        "The code below utilizes an LLM and uses langchain and redis database for memory based context retrieval and understanding. What it does, is that user can ask a question, and that question is stored in Redis and its answer, with the `username` as the unique identifier.\n",
        "\n",
        "Then, when we again ask a question that is related to old information, all context for that particular `username` is retireved and sent as context to the LLM.\n",
        "\n",
        "However, a drawback emerges during prolonged conversations, as more messages accumulate in Redis for a specific user. This leads to the retrieval of the entire message history when fetching context information for that username. Consequently, sending this extensive context information to the LLM becomes costlier, as opposed to a more efficient approach of providing only the most relevant and similar documents to address the user's question."
      ],
      "metadata": {
        "id": "gBoi-qRit_l-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"\"\"The following is a friendly conversation between a human and an AI.\n",
        "The AI is talkative and provides lots of specific details from its context.\n",
        "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
        "\n",
        "(Note that you do not need to use these pieces of information if not relevant)\n",
        "\"\"\"),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | ChatOpenAI()\n",
        "\n",
        "chain_with_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    lambda session_id: RedisChatMessageHistory(\n",
        "        session_id, url=os.getenv('REDIS_URL')\n",
        "    ),\n",
        "    input_messages_key=\"question\",\n",
        "    history_messages_key=\"history\",\n",
        ")\n",
        "\n",
        "config = {\"configurable\": {\"session_id\": \"INSERT_USERNAME_HERE\"}}\n"
      ],
      "metadata": {
        "id": "knUP5WeCtLAR"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "chain_with_history.invoke({\"question\": \"What did i ask about taylor swffit?\"}, config=config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPRAWTd5t07i",
        "outputId": "a8e48420-0402-4815-8eff-95b1f79dc286"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I apologize for any confusion, but as an AI, I do not have access to your previous questions or conversations. Therefore, I cannot recall what you specifically asked about Taylor Swffit. If you can provide me with more details or repeat your question, I'll be happy to help you with any information I have available.\")"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Vector Store Retriever Memory\n",
        "\n",
        "\n",
        "This method will use Redis as a vector database. All user conversations are stored in the Redis DB with the `username` as the unique identifier, and then all those messages are turned into vectors and those vectors are also stored in the database.\n",
        "\n",
        "\n",
        "Now when user enters a query, the system will turn it into a vector, query the database to find the top n most similar vectors, and sends them to the LLM as context. The LLM can then answer from the context. If the user query doesn't match any vectors in the database, the LLM will then make up a general answer itself. üòÅüòÅ"
      ],
      "metadata": {
        "id": "goR7Np5Wl4wS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain_openai import OpenAI\n",
        "from langchain.memory import VectorStoreRetrieverMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.vectorstores.redis import Redis\n",
        "\n",
        "embedding_fn = OpenAIEmbeddings()\n",
        "index_name=\"USER1\""
      ],
      "metadata": {
        "id": "vvwcpDQyl8YY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# metadata = [\n",
        "#     {\n",
        "#         \"username\": \"john\",\n",
        "#         \"uuid\": '3JBK4K24C#',\n",
        "#     },\n",
        "# ]\n"
      ],
      "metadata": {
        "id": "C1_ZanfhrvLD"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.1. Create a new user index (username)"
      ],
      "metadata": {
        "id": "Ay1CBq96IbNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rds = Redis.from_texts(\n",
        "    texts = [\"Hi there\"],\n",
        "    embedding=embedding_fn,\n",
        "    # metadatas=metadata,\n",
        "    redis_url=os.getenv(\"REDIS_URL\"),\n",
        "    index_name=index_name\n",
        ")"
      ],
      "metadata": {
        "id": "6oCw2JGJnCds"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rds.write_schema(\"redis_schema.yaml\")\n",
        "# rds.schema"
      ],
      "metadata": {
        "id": "K9IvPB6NqlQv"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Load all existing documents from a given index (username)"
      ],
      "metadata": {
        "id": "LrEqtxNlIee9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_rds = Redis.from_existing_index(\n",
        "    embedding=embedding_fn,\n",
        "    index_name=\"USER1\",\n",
        "    redis_url=os.getenv(\"REDIS_URL\"),\n",
        "    schema=rds.schema,\n",
        ")\n",
        "\n",
        "# results = new_rds.similarity_search('moaaz', k=3)"
      ],
      "metadata": {
        "id": "ViGxB35Wqlhj"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3. Use the redis database as the vector store"
      ],
      "metadata": {
        "id": "xDyWRVBgI2eP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = rds.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
        "memory = VectorStoreRetrieverMemory(retriever=retriever)"
      ],
      "metadata": {
        "id": "7VviHbNyqxFx"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4. Select the preferred LLM, either open source or propreitary"
      ],
      "metadata": {
        "id": "ZeahIo1HJHrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# llm = OpenAI(temperature=0.6)\n",
        "\n",
        "\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id='google/flan-t5-large', model_kwargs={\"temperature\": 0.7, \"max_length\": 256}\n",
        ")\n"
      ],
      "metadata": {
        "id": "kFeHzYoH8baE"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5. Set prompt template"
      ],
      "metadata": {
        "id": "cvB8KpcAJRyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_DEFAULT_TEMPLATE = \"\"\"The following is a friendly conversation between a human and an AI.\n",
        "The AI is talkative and provides lots of specific details from its context.\n",
        "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
        "\n",
        "Relevant pieces of previous conversation:\n",
        "{history}\n",
        "\n",
        "(Note that you do not need to use these pieces of information if not relevant)\n",
        "\n",
        "Current conversation:\n",
        "Human: {input}\n",
        "AI:\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    input_variables=[\"history\", \"input\"], template=_DEFAULT_TEMPLATE\n",
        ")\n"
      ],
      "metadata": {
        "id": "5NouMnzRr-OE"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6. Create conversation chain"
      ],
      "metadata": {
        "id": "xyAlN2KnJWQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_summary = ConversationChain(\n",
        "    llm=llm,\n",
        "    prompt=PROMPT,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "He1JE1nfJYZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.7. Talk with context!"
      ],
      "metadata": {
        "id": "BNFMPkF_JZn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_with_summary.predict(input=\"How are you?\")\n"
      ],
      "metadata": {
        "id": "rKP7xHRhyQE9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}